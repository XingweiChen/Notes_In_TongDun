{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "spark = SparkSession.builder.appName('create_table_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleData = [(\"bob\",\"Developer\",125000),\n",
    "              (\"mark\",\"Developer\",108000),\n",
    "              (\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),\n",
    "              (\"jon\",\"Tester\",65000),\n",
    "              (\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),\n",
    "              (\"eric\",\"Developer\",144000),\n",
    "              (\"carlos\",\"Tester\",75000),\n",
    "              (\"henry\",\"Developer\",110000)]\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('job', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|125000|\n",
      "|108000|\n",
      "| 70000|\n",
      "|185000|\n",
      "| 65000|\n",
      "| 82000|\n",
      "| 98000|\n",
      "|144000|\n",
      "| 75000|\n",
      "|110000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleData = [(\"bob\",\"Developer\",125000),\n",
    "              (\"mark\",\"Developer\",108000),\n",
    "              (\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),\n",
    "              (\"jon\",\"Tester\",65000),\n",
    "              (\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),\n",
    "              (\"eric\",\"Developer\",144000),\n",
    "              (\"carlos\",\"Tester\",75000),\n",
    "              (\"henry\",\"Developer\",110000)]\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df = df.drop('name', 'job')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bob</td>\n",
       "      <td>Developer</td>\n",
       "      <td>125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark</td>\n",
       "      <td>Developer</td>\n",
       "      <td>108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carl</td>\n",
       "      <td>Tester</td>\n",
       "      <td>70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peter</td>\n",
       "      <td>Developer</td>\n",
       "      <td>185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jon</td>\n",
       "      <td>Tester</td>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roman</td>\n",
       "      <td>Tester</td>\n",
       "      <td>82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>simon</td>\n",
       "      <td>Developer</td>\n",
       "      <td>98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eric</td>\n",
       "      <td>Developer</td>\n",
       "      <td>144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>carlos</td>\n",
       "      <td>Tester</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>henry</td>\n",
       "      <td>Developer</td>\n",
       "      <td>110000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name        job  salary\n",
       "0     bob  Developer  125000\n",
       "1    mark  Developer  108000\n",
       "2    carl     Tester   70000\n",
       "3   peter  Developer  185000\n",
       "4     jon     Tester   65000\n",
       "5   roman     Tester   82000\n",
       "6   simon  Developer   98000\n",
       "7    eric  Developer  144000\n",
       "8  carlos     Tester   75000\n",
       "9   henry  Developer  110000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleData = [(\"bob\",\"Developer\",125000),\n",
    "              (\"mark\",\"Developer\",108000),\n",
    "              (\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),\n",
    "              (\"jon\",\"Tester\",65000),\n",
    "              (\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),\n",
    "              (\"eric\",\"Developer\",144000),\n",
    "              (\"carlos\",\"Tester\",75000),\n",
    "              (\"henry\",\"Developer\",110000)]\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"]).toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataframe line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob\n",
      "mark\n",
      "carl\n",
      "peter\n",
      "jon\n",
      "roman\n",
      "simon\n",
      "eric\n",
      "carlos\n",
      "henry\n"
     ]
    }
   ],
   "source": [
    "sampleData = [(\"bob\",\"Developer\",125000),\n",
    "              (\"mark\",\"Developer\",108000),\n",
    "              (\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),\n",
    "              (\"jon\",\"Tester\",65000),\n",
    "              (\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),\n",
    "              (\"eric\",\"Developer\",144000),\n",
    "              (\"carlos\",\"Tester\",75000),\n",
    "              (\"henry\",\"Developer\",110000)]\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "for line in df.collect():\n",
    "    print( line[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n",
      "|  name|      job|salary|\n",
      "+------+---------+------+\n",
      "|   bob|Developer|125000|\n",
      "|  mark|Developer|108000|\n",
      "|  carl|   Tester| 70000|\n",
      "| peter|Developer|185000|\n",
      "|   jon|   Tester| 65000|\n",
      "| roman|   Tester| 82000|\n",
      "| simon|Developer| 98000|\n",
      "|  eric|Developer|144000|\n",
      "|carlos|   Tester| 75000|\n",
      "| henry|Developer|110000|\n",
      "+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fill extra column with constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-------+\n",
      "|  name|      job|salary|version|\n",
      "+------+---------+------+-------+\n",
      "|   bob|Developer|125000|    100|\n",
      "|  mark|Developer|108000|    100|\n",
      "|  carl|   Tester| 70000|    100|\n",
      "| peter|Developer|185000|    100|\n",
      "|   jon|   Tester| 65000|    100|\n",
      "| roman|   Tester| 82000|    100|\n",
      "| simon|Developer| 98000|    100|\n",
      "|  eric|Developer|144000|    100|\n",
      "|carlos|   Tester| 75000|    100|\n",
      "| henry|Developer|110000|    100|\n",
      "+------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df = df.withColumn('version', lit(100))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-----+---------+-------+\n",
      "|  name|      job|salary|name2|     job2|salary2|\n",
      "+------+---------+------+-----+---------+-------+\n",
      "| peter|Developer|185000|peter|Developer| 185000|\n",
      "| simon|Developer| 98000| null|     null|   null|\n",
      "|carlos|   Tester| 75000| null|     null|   null|\n",
      "|  eric|Developer|144000| null|     null|   null|\n",
      "| roman|   Tester| 82000| null|     null|   null|\n",
      "|   jon|   Tester| 65000| null|     null|   null|\n",
      "|  mark|Developer|108000| mark|Developer| 108000|\n",
      "|  carl|   Tester| 70000| carl|   Tester|  70000|\n",
      "|   bob|Developer|125000|  bob|Developer| 125000|\n",
      "+------+---------+------+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sampleData2 = [(\"bob\",\"Developer\",125000),\n",
    "              (\"mark\",\"Developer\",108000),\n",
    "              (\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000)]\n",
    "\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df2 = spark.createDataFrame(sampleData2, schema = [\"name2\", \"job2\", \"salary2\"])\n",
    "df = df.join(df2, col('name') == col('name2'), 'left')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-----+---------+-------+\n",
      "|  name|      job|salary|name2|     job2|salary2|\n",
      "+------+---------+------+-----+---------+-------+\n",
      "| henry|Developer|110000| null|     null|      0|\n",
      "| peter|Developer|185000|peter|Developer| 185000|\n",
      "| simon|Developer| 98000| null|     null|      0|\n",
      "|carlos|   Tester| 75000| null|     null|      0|\n",
      "|  eric|Developer|144000| null|     null|      0|\n",
      "| roman|   Tester| 82000| null|     null|      0|\n",
      "|   jon|   Tester| 65000| null|     null|      0|\n",
      "|  mark|Developer|108000| mark|Developer| 108000|\n",
      "|  carl|   Tester| 70000| carl|   Tester|  70000|\n",
      "|   bob|Developer|125000|  bob|Developer| 125000|\n",
      "+------+---------+------+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['a', 1], ['b', 2]]\n",
    "import json\n",
    "b = json.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = json.loads(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[476] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"]).rdd\n",
    "df2 = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"]).rdd\n",
    "df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df1.map(lambda x: [(x[0], x[1], x[2]), 1]).reduceByKey(lambda x1, x2: x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('bob', 'Developer', 125000), 1), (('mark', 'Developer', 108000), 1)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))#.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bob', 'Developer', 125000, 1),\n",
       " ('mark', 'Developer', 108000, 1),\n",
       " ('carl', 'Tester', 70000, 1),\n",
       " ('peter', 'Developer', 185000, 1),\n",
       " ('jon', 'Tester', 65000, 1),\n",
       " ('roman', 'Tester', 82000, 1),\n",
       " ('simon', 'Developer', 98000, 1),\n",
       " ('eric', 'Developer', 144000, 1),\n",
       " ('carlos', 'Tester', 75000, 1),\n",
       " ('henry', 'Developer', 110000, 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caching is very useful for **applications that re-use an RDD multiple times.**\n",
    "* **Caching all of the generated RDDs is not a good strategy** as useful cached blocks may be evicted from the cache well before being re-used. For such cases, additional computation time is required to re-evaluate the RDD blocks evicted from the cache.\n",
    "* **Given a large list of RDDs that are being used multiple times, deciding which ones to cache may be challenging. When memory is scarce, it is recommended to use MEMORY_AND_DISK caching strategy** such that evicted blocks from cache are saved to disk. Reading the blocks from disk is generally faster than re-evaluation. If extra processing cost can be afforded, MEMORY_AND_DISK_SER can further reduce the memory footprint of the cached RDDs.\n",
    "* If certain RDDs have very large evaluation cost, it is recommended to replicate them to another node. This will boost significantly performance in the case of a node failure, since re-evaluation can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count result 500000\n",
      "38.809606313705444\n",
      "Count result 500000\n",
      "46.60890793800354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "time1 = time.time()\n",
    "pairs1 = spark.sparkContext.parallelize([x for x in range(1000000)], 10).flatMap( lambda x: [(x//2, x)]\n",
    "                                                                ).cache()\n",
    "\n",
    "for i in range(100):\n",
    "    pairs1.count()\n",
    "print(\"Count result %s\"% pairs1.groupByKey().count())\n",
    "time2 = time.time()\n",
    "print(time2-time1)\n",
    "\n",
    "time1 = time.time()\n",
    "pairs1 = spark.sparkContext.parallelize([x for x in range(1000000)], 10).flatMap( lambda x: [(x//2, x)]\n",
    "                                                                )\n",
    "for i in range(100):\n",
    "    pairs1.count()\n",
    "print(\"Count result %s\"% pairs1.groupByKey().count())\n",
    "time2 = time.time()\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approxQuantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n",
      "|  Name|     Role|Salary|\n",
      "+------+---------+------+\n",
      "|   bob|Developer|125000|\n",
      "|  mark|Developer|108000|\n",
      "|  carl|   Tester| 70000|\n",
      "| peter|Developer|185000|\n",
      "|   jon|   Tester| 65000|\n",
      "| roman|   Tester| 82000|\n",
      "| simon|Developer| 98000|\n",
      "|  eric|Developer|144000|\n",
      "|carlos|   Tester| 75000|\n",
      "+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL Hive integration example\").getOrCreate()\n",
    "sampleData = [(\"bob\",\"Developer\",125000),(\"mark\",\"Developer\",108000),(\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),(\"jon\",\"Tester\",65000),(\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),(\"eric\",\"Developer\",144000),(\"carlos\",\"Tester\",75000)]\n",
    "df = spark.createDataFrame(sampleData, schema=[\"Name\",\"Role\",\"Salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185000, 144000, 125000, 108000, 98000, 82000, 75000, 70000, 65000]\n",
      "中位数: [98000.0]\n",
      "四分位数: [75000.0]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(sampleData, schema=[\"Name\",\"Role\",\"Salary\"])\n",
    "print( [x[0] for x in sorted(df.select('Salary').collect(), reverse=True)] ) \n",
    "print('中位数: %s' % df.approxQuantile(\"Salary\", [0.5], 0.01)) # 第三个值代表误差\n",
    "print('四分位数: %s' % df.approxQuantile(\"Salary\", [0.25], 0.01)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapPartitions\n",
    "**mapPartitions transformation is faster than map since it calls your function once/partition, not once/element.**\n",
    "* Example Scenario : \n",
    "    * if we have 100K elements in a particular RDD partition then we will fire off the function being used by the mapping transformation 100K times when we use map.\n",
    "\n",
    "    * Conversely, if we use mapPartitions then we will only call the particular function one time, but we will pass in all 100K records and get back all responses in one function call.\n",
    "\n",
    "    * There will be performance gain since map works on a particular function so many times, especially if the function is doing something expensive each time that it wouldn't need to do if we passed in all the elements at once(in case of mappartitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n",
      "|    _1|       _2|    _3|\n",
      "+------+---------+------+\n",
      "|  carl|   Tester| 70000|\n",
      "| roman|   Tester| 82000|\n",
      "|carlos|   Tester| 75000|\n",
      "|   bob|Developer|125000|\n",
      "| peter|Developer|185000|\n",
      "| simon|Developer| 98000|\n",
      "| henry|Developer|110000|\n",
      "|  mark|Developer|108000|\n",
      "|   jon|   Tester| 65000|\n",
      "|  eric|Developer|144000|\n",
      "+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(part):\n",
    "    result = []\n",
    "    for row in part:\n",
    "        result.append(list(row))\n",
    "    return result\n",
    "sampleData = [(\"bob\",\"Developer\",125000),(\"mark\",\"Developer\",108000),(\"carl\",\"Tester\",70000),\n",
    "              (\"peter\",\"Developer\",185000),(\"jon\",\"Tester\",65000),(\"roman\",\"Tester\",82000),\n",
    "              (\"simon\",\"Developer\",98000),(\"eric\",\"Developer\",144000),(\"carlos\",\"Tester\",75000),\n",
    "              (\"henry\",\"Developer\",110000)]\n",
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df = df.repartition(3)\n",
    "df = df.rdd.mapPartitions(func).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n",
      "|    _1|       _2|    _3|\n",
      "+------+---------+------+\n",
      "|   bob|Developer|125000|\n",
      "|  mark|Developer|108000|\n",
      "|  carl|   Tester| 70000|\n",
      "| peter|Developer|185000|\n",
      "|   jon|   Tester| 65000|\n",
      "| roman|   Tester| 82000|\n",
      "| simon|Developer| 98000|\n",
      "|  eric|Developer|144000|\n",
      "|carlos|   Tester| 75000|\n",
      "| henry|Developer|110000|\n",
      "+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(sampleData, schema = [\"name\", \"job\", \"salary\"])\n",
    "df = df.coalesce(3)\n",
    "df = df.rdd.mapPartitions(func).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare repartitions and coalesce\n",
    "### coalesce\n",
    "The coalesce avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, only moving the data off the extra nodes, onto the nodes that we kept.\n",
    "\n",
    "So, it would go something like this:\n",
    "```\n",
    "Node 1 = 1,2,3\n",
    "Node 2 = 4,5,6\n",
    "Node 3 = 7,8,9\n",
    "Node 4 = 10,11,12\n",
    "```\n",
    "Then coalesce down to 2 partitions:\n",
    "```\n",
    "Node 1 = 1,2,3 + (10,11,12)\n",
    "Node 3 = 7,8,9 + (4,5,6)\n",
    "```\n",
    "### repartitions\n",
    "The repartition algorithm does a full shuffle and creates new partitions with data that's distributed evenly. Let's create a DataFrame with the numbers from 1 to 12.\n",
    "```\n",
    "val x = (1 to 12).toList\n",
    "val numbersDf = x.toDF(\"number\")\n",
    "```\n",
    "numbersDf contains 4 partitions on my machine.\n",
    "numbersDf.rdd.partitions.size // => 4\n",
    "Here is how the data is divided on the partitions:\n",
    "```\n",
    "Partition 00000: 1, 2, 3\n",
    "Partition 00001: 4, 5, 6\n",
    "Partition 00002: 7, 8, 9\n",
    "Partition 00003: 10, 11, 12\n",
    "```\n",
    "Let's do a full-shuffle with the repartition method and get this data on two nodes.\n",
    "```\n",
    "val numbersDfR = numbersDf.repartition(2)\n",
    "```\n",
    "Here is how the numbersDfR data is partitioned on my machine:\n",
    "```\n",
    "Partition A: 1, 3, 4, 6, 7, 9, 10, 12\n",
    "Partition B: 2, 5, 8, 11\n",
    "```\n",
    "The repartition method makes new partitions and evenly distributes the data in the new partitions (the data distribution is more even for larger data sets).\n",
    "\n",
    "** Difference between coalesce and repartition **\n",
    "\n",
    "coalesce uses existing partitions to minimize the amount of data that's shuffled.  repartition creates new partitions and does a full shuffle.  coalesce results in partitions with different amounts of data (sometimes partitions that have much different sizes) and repartition results in roughly equal sized partitions.\n",
    "\n",
    "** Is coalesce or repartition faster? **\n",
    "\n",
    "coalesce may run faster than repartition, but unequal sized partitions are generally slower to work with than equal sized partitions. You'll usually need to repartition datasets after filtering a large data set. I've found repartition to be faster overall because Spark is built to work with equal sized partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select和indexing\n",
    "* df[\"xxx\"] 和 df.select(\"xxx\") 不同select返回的是DataFrame，直接indexing返回的是column对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string] \t Column<b'name'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "schema = StructType([StructField(\"name\", StringType(), True),\n",
    "                     StructField(\"age\", IntegerType(), True),\n",
    "                     StructField(\"salary\", DoubleType(), True)])\n",
    "df = spark.createDataFrame(data=[(\"a\", 1, 100.0), (\"b\", 2, 200.0)], schema=schema)\n",
    "print( df.select(\"name\"), '\\t', df[\"name\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 返回DataFrame的基本统计数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+-----------------+\n",
      "|summary|name|               age|           salary|\n",
      "+-------+----+------------------+-----------------+\n",
      "|  count|   2|                 2|                2|\n",
      "|   mean|null|               1.5|            150.0|\n",
      "| stddev|null|0.7071067811865476|70.71067811865476|\n",
      "|    min|   a|                 1|            100.0|\n",
      "|    max|   b|                 2|            200.0|\n",
      "+-------+----+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|  null|\n",
      "|emp2| null|  null|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = [('emp1', 'John', None), ('emp2', None, None), \n",
    "              ('emp1', None, 345.0), ('emp1', 'Cindy', 456.0)], schema = StructType([StructField(\"pos\", StringType(), True),\n",
    "     StructField(\"name\", StringType(), True),\n",
    "     StructField(\"salary\", DoubleType(), True)]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|  null|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+----------+------+\n",
      "| pos|      name|salary|\n",
      "+----+----------+------+\n",
      "|emp1|      John|  null|\n",
      "|emp2|FILL VALUE|  null|\n",
      "|emp1|FILL VALUE| 345.0|\n",
      "|emp1|     Cindy| 456.0|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill('FILL VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|   0.0|\n",
      "|emp2| null|   0.0|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "| pos|   name|salary|\n",
      "+----+-------+------+\n",
      "|emp1|   John|  null|\n",
      "|emp2|No name|  null|\n",
      "|emp1|No name| 345.0|\n",
      "|emp1|  Cindy| 456.0|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill('No name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John| 400.5|\n",
      "|emp2| null| 400.5|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.na.fill( df.select(mean(df['salary'])).collect()[0][0], subset=['salary'] ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据进行shuffle操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "|  a|  1| 18|\n",
      "|  b|  2| 22|\n",
      "|  c|  3| 20|\n",
      "|  d|  4| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = [('a', 1, 18), ('b', 2, 22), ('c', 3, 20), ('d', 4, 10)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "|  b|  2| 22|\n",
      "|  a|  1| 18|\n",
      "|  d|  4| 10|\n",
      "|  c|  3| 20|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "shuffledDF = df.orderBy(rand())\n",
    "shuffledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
