{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train(T+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/bin/python2\n",
    "# set spark.driver.maxResultSize=10g\n",
    "# set spark.driver.memory=20g\n",
    "# set spark.driver.memoryOverhead=12g\n",
    "# set spark.executor.memory=25g\n",
    "# set spark.executor.memoryOverhead=12g\n",
    "# set spark.sql.execution.arrow.enabled=true\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, ArrayType, LongType, FloatType\n",
    "import pyspark.sql.functions as F \n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以最大的窗口向后获取数据，供给pagerank计算node的流出各边的权重\n",
    "输入:\n",
    "    sdf: 从千维度表中筛选有用columns后，得到的小表\n",
    "    window: 窗口大小，单位为秒\n",
    "输出:\n",
    "    表: [平台名，时间差，窗口内的下一条记录的平台名]\n",
    "\"\"\"\n",
    "def forward_for_pr(spark, sdf, window=420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func(row):\n",
    "        result = []\n",
    "        # 按时间，再按sequence_id排序，避免统一时间的记录的排序不稳定\n",
    "        items = sorted(row[1], key=lambda x: (x[1], x[0]))\n",
    "        for i in range(1, len(items)):\n",
    "            if items[i][1] - items[i-1][1] <= bc_window.value:\n",
    "                if items[i][3] != items[i-1][3]:\n",
    "                    result.append((items[i-1][3], items[i][1] - items[i-1][1], items[i][3]))\n",
    "            else:\n",
    "                result.append((items[i-1][3], -1.0, ''))\n",
    "        return result\n",
    "    schema = StructType([StructField('partnercode', StringType(),True),\n",
    "                         StructField('delta_time', FloatType(),True),\n",
    "                         StructField('fromplat', StringType(),True)])\n",
    "    # 按身份证聚合，然后按时间和流水号排序，将时间间隔在窗口内的数据作为关联数据，其余为单独数据\n",
    "    sdf = sdf.rdd.map( lambda x: ( x.rqst_idnumber, x ) ).groupByKey().flatMap(func).toDF(schema)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以最大的窗口向前获取数据，供给fp-growth等计算关联，因为我们希望将A->B中的B去除，优势在于B可能有多条\n",
    "输入:\n",
    "    sdf: 从千维度表中筛选有用columns后，得到的小表\n",
    "    window: 窗口大小，单位为秒\n",
    "输出:\n",
    "    表: [流水号，发生时间，身份证号，平台名，设备码，用户标记的引流标识，时间差，窗口内的上一条记录的平台名]\n",
    "\"\"\"\n",
    "def backward_for_fp(spark, sdf, window=420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func1(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            result.append([ row[2], list(row) ])\n",
    "        return result\n",
    "    \n",
    "    def func2(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            items = sorted(list(row[1]), key=lambda x: (x[1], x[0]))\n",
    "            for i in range(1, len(items)):\n",
    "                if items[i][1] - items[i-1][1] <= bc_window.value:\n",
    "                    if items[i][3] != items[i-1][3]:\n",
    "                        result.append( items[i] + [ int(items[i][1] - items[i-1][1]), items[i-1][3] ])\n",
    "                else:\n",
    "                    result.append( items[i] + [ -1, '' ] )\n",
    "        return result\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"eventoccurtime\", FloatType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"deviceid\", StringType(), True),\n",
    "        StructField(\"rqst_customerchannel\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"items\", StringType(), True)])\n",
    "    sdf = sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions( func1 ).groupByKey().mapPartitions( func2 ).toDF(schema)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pagerank算法部分\n",
    "输入:     sdf: forward_for_pr的输出\n",
    "         当前模型的版本号\n",
    "输出:\n",
    "         写入pgrk_result[rank, support_cnt, outdegree]\n",
    "         \n",
    "'''\n",
    "def pagerank(spark, sdf, version_number, error_thresh = 1e-2, trans_thresh = 0.85, thresh1=0, thresh2=0.001, thresh3=100, window = 420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            if row.delta_time < 0 or row.delta_time > bc_window.value:\n",
    "                result.append((row.partnercode, ''))\n",
    "            else:\n",
    "                result.append((row.partnercode, row.fromplat))\n",
    "        return result\n",
    "   \n",
    "    sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions(func)\n",
    "    thresh1 = spark.sparkContext.broadcast(thresh1)\n",
    "    thresh2 = spark.sparkContext.broadcast(thresh2)\n",
    "\n",
    "    # filter with thresh1 and thresh2\n",
    "    def map_to_plat_pair(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            if row[1]:\n",
    "                result.append([ (row[0], row[1]), 1 ])\n",
    "            else:\n",
    "                # if not drainage append it call itself\n",
    "                result.append([ (row[0], row[0]), 1])\n",
    "        return result\n",
    "\n",
    "    def cnt_func(row):\n",
    "        cnt1, cnt2 = 0, 0\n",
    "        for item in row[1]: # filter by first thresh, count total\n",
    "            cnt1 += 0 if item[1] < thresh1.value else item[1]\n",
    "        for item in row[1]: # filter by second thresh * count_total\n",
    "            cnt2 += 0 if item[1] < cnt1 * thresh2.value else item[1]\n",
    "        return [row[0], cnt2]\n",
    "    \n",
    "    # get record count for each platform \n",
    "    plat_support_cnt_dict =  spark.sparkContext.broadcast( dict( sdf.mapPartitions(map_to_plat_pair).reduceByKey(lambda x1, x2: x1+x2).groupBy(lambda x: x[0][0]).map(\n",
    "        cnt_func).collect() ) )\n",
    "    # filter with thresh3\n",
    "    tmp = []\n",
    "    for item in sdf.map(lambda x: (x[0], '')).reduceByKey(lambda x1, x2: x1).collect():\n",
    "        if plat_support_cnt_dict.value[item[0]] >= thresh3:\n",
    "            tmp.append(item[0])\n",
    "    # get plat to index matching dictionary\n",
    "    bc_plat_idx = spark.sparkContext.broadcast(dict(zip(tmp, range(len(tmp)))))\n",
    "    bc_length = spark.sparkContext.broadcast(len(tmp))\n",
    "\n",
    "    # function for calculate transform matrix\n",
    "    def calc_transfer_matrix(row):\n",
    "        if row[0] not in bc_plat_idx.value:\n",
    "            return []\n",
    "        result = [float(0) for _ in range(bc_length.value)]\n",
    "        cnt = 0.0\n",
    "        for item in row[1]:\n",
    "            if item[1] < plat_support_cnt_dict.value[row[0]] * thresh2.value or item[1] < thresh1.value or item[0][1] not in bc_plat_idx.value: continue #\n",
    "            cnt += item[1]\n",
    "        for item in row[1]:\n",
    "            if item[1] < plat_support_cnt_dict.value[row[0]] * thresh2.value or item[1] < thresh1.value or item[0][1] not in bc_plat_idx.value: continue #\n",
    "            result[bc_plat_idx.value[item[0][1]]] = float(item[1]) / cnt\n",
    "        if np.sum(result) == 0:\n",
    "            result[bc_plat_idx.value[row[0]]] = 1.0\n",
    "        return [(row[0], 1, result)]\n",
    "    # calculate out precentage for each out going edge\n",
    "    df_rdd = sdf.mapPartitions(map_to_plat_pair).reduceByKey(lambda x1, x2: x1+x2).groupBy(lambda x: x[0][0]).flatMap(\n",
    "        calc_transfer_matrix)\n",
    "    # function for transfermation\n",
    "    def pg_transfer(row):\n",
    "        return [ [ bc_idx_plat.value[i], \n",
    "                   [ bc_beta.value * (row[1] * row[2][i]), [] ] ] \n",
    "                 for i in range(len(row[2])) ] + [ [row[0], [1-bc_beta.value, row[2]]] ]\n",
    "    \n",
    "    # get and save idx to platform dictionary\n",
    "    bc_idx_plat = spark.sparkContext.broadcast(dict([(v, k) for k, v in bc_plat_idx.value.items()]))\n",
    "    # iterate until error change less than thresh4\n",
    "    bc_beta = spark.sparkContext.broadcast(trans_thresh)\n",
    "    old_error, new_error, newdf_rdd = sys.maxint, sys.maxint - 1, df_rdd\n",
    "    if(len(df_rdd.collect())) == 0:\n",
    "        raise('all data is filtered!!!')\n",
    "    while(old_error - new_error > error_thresh):\n",
    "        df_rdd, old_error = newdf_rdd, new_error\n",
    "        new_result = df_rdd.flatMap(pg_transfer).reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])\n",
    "        old_result = df_rdd.map(lambda x: [x[0], [x[1], x[2]] ])\n",
    "        result = new_result.union(old_result)\n",
    "        diff = result.map(lambda x: (x[0], x[1][0])).reduceByKey(lambda x1, x2: (x1 - x2) ** 2)\n",
    "        new_error = math.sqrt( diff.map(lambda x: ('', x[1])).reduceByKey(lambda x1, x2: x1 + x2).collect()[0][1] )\n",
    "        print('error before: %.6f, error after: %.6f' % (old_error, new_error))\n",
    "        newdf_rdd = new_result.map(lambda x: [ x[0], x[1][0], x[1][1] ])\n",
    "    print(new_error)\n",
    "    \n",
    "    # merge result with plat cnt\n",
    "    newdf_rdd = newdf_rdd.map(lambda x: [ x[0], x[1], x[2], plat_support_cnt_dict.value[x[0]] ])\n",
    "    platform_dict = spark.sparkContext.broadcast( dict(newdf_rdd.map(lambda x: (x[0], x[1])).collect()) )\n",
    "    \n",
    "    # translate linked platform idx to name\n",
    "    def mapping_sort(row):\n",
    "        result = 0\n",
    "        for i in range(len(row[2])):\n",
    "            if row[2][i] > 0:\n",
    "                plt = bc_idx_plat.value[i]\n",
    "                # result.append( [plt, row[2][i]] )\n",
    "                result += 1 if row[2][i] > 0.01 else 0\n",
    "        # result = sorted(result, key = lambda x: x[1], reverse=True)\n",
    "        return ( row[0], row[1], row[3], result ) \n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"rank\", DoubleType(), True),\n",
    "        StructField(\"count\", LongType(), True),\n",
    "        StructField(\"infos\", IntegerType(), True) ])    \n",
    "\n",
    "    # output result as table\n",
    "    newdf = newdf_rdd.map(mapping_sort).toDF(schema)\n",
    "    bc_pr_dict = spark.sparkContext.broadcast( dict( newdf.rdd.map( lambda x: (x[0], [ x[1], x[2], x[3] ]) ).collect() ) )\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    newdf.registerTempTable(tmp_table_name)\n",
    "    spark.sql( 'insert overwrite table ml.mlp_cxw_more_%s_pgrk_result_dt partition (ds) select *, %s from %s' % (str(int(window)), version_number, tmp_table_name) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "计算条件概率, ir, kulc, lift, window/global\n",
    "ir = P(A|B) / P(B|A)\n",
    "kulc = 0.5 * (P(A|B) + P(B|A))\n",
    "lift = P(A|B) / P(A)\n",
    "window / global = cnt in window / cnt in global\n",
    "输入:\n",
    "        sdf: backward_for_fp的输出\n",
    "        version_number\n",
    "输出:\n",
    "        永久表:\n",
    "            bc_dict\n",
    "            cond_prob\n",
    "            win_over_all_ratio\n",
    "            合并所有特征后的对于当前window的大表\n",
    "'''\n",
    "def fp_growth_table(spark, sdf, version_number, min_support = 100, window=420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            if row.delta_time < 0 or row.delta_time > bc_window.value:\n",
    "                result.append(list(row[:-2]) + [-1, ''])\n",
    "            else:\n",
    "                # 分箱\n",
    "                ttime = 6\n",
    "                if row[-2] <= 20:\n",
    "                    ttime = 0\n",
    "                elif row[-2] <= 45:\n",
    "                    ttime = 1\n",
    "                elif row[-2] <= 70:\n",
    "                    ttime = 2\n",
    "                elif row[-2] <= 120:\n",
    "                    ttime = 3\n",
    "                elif row[-2] <= 180:\n",
    "                    ttime = 4\n",
    "                elif row[-2] <= 300:\n",
    "                    ttime = 5\n",
    "                result.append( list(row[:-2]) + [ttime, row[-1]] )\n",
    "        return result\n",
    "   \n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"eventoccurtime\", FloatType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"deviceid\", StringType(), True),\n",
    "        StructField(\"rqst_customerchannel\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"items\", StringType(), True)])\n",
    "    sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions(func).toDF(schema)\n",
    "    bc_support_count = spark.sparkContext.broadcast(min_support)\n",
    "    def items_frequency(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            result.append( ( (row[3], row[-1]), 1 ) )\n",
    "            if len(row[-1]) > 0:\n",
    "                result.append( ( (row[-1], ''), 1 ) )\n",
    "                result.append( ( (row[3], ''), 1 ) )\n",
    "        return result\n",
    "\n",
    "    def items_filter(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            if row[1] >= bc_support_count.value:\n",
    "                result.append( ( row[0], row[1] ) )\n",
    "        return result\n",
    "    # 计算item的频数\n",
    "    sdf.repartition(100)\n",
    "    df_freq = sdf.rdd.mapPartitions(items_frequency).reduceByKey(lambda x1, x2: x1 + x2).mapPartitions(items_filter)\n",
    "    \n",
    "    freq_dict = dict( df_freq.map(lambda x: (','.join(sorted(x[0])), x[1]) if len(x[0][1]) > 0 else (x[0][0], x[1])).collect() )\n",
    "    candidates_list = spark.sparkContext.broadcast( set( freq_dict.keys() ) )\n",
    "    freq_dict['totol_count_of_platforms'] = int( np.sum( list( freq_dict.values() ) ) )\n",
    "    bc_dict = spark.sparkContext.broadcast( freq_dict )\n",
    "    \n",
    "    tmp_df = spark.createDataFrame(bc_dict.value.items())\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    tmp_df.registerTempTable(tmp_table_name)\n",
    "    spark.sql( 'insert overwrite table ml.mlp_cxw_more_%s_bc_dict_dt partition (ds) select *, %s from %s' % (str(int(window)), version_number, tmp_table_name) )\n",
    "\n",
    "    # 计算win_over_all\n",
    "    def count_global(row):\n",
    "        cands_set = sorted(list(set(row[1])))\n",
    "        result = []\n",
    "        for i in range(len(cands_set)):\n",
    "            for j in range(i+1, len(cands_set)):\n",
    "                tmp_str = cands_set[i] + ',' + cands_set[j]\n",
    "                if tmp_str in candidates_list.value:\n",
    "                    result.append( (tmp_str, 1) )\n",
    "        return result\n",
    "    \n",
    "    def count_window(row):\n",
    "        tmp_set = set(row[1])\n",
    "        if '' in tmp_set:\n",
    "            tmp_set.remove('')\n",
    "        cands_set = sorted(list(tmp_set))\n",
    "        result = []\n",
    "        for i in range(len(cands_set)):\n",
    "            for j in range(i+1, len(cands_set)):\n",
    "                tmp_str = cands_set[i] + ',' + cands_set[j]\n",
    "                if tmp_str in candidates_list.value:\n",
    "                    result.append( (tmp_str, 1) )\n",
    "        return result\n",
    "    \n",
    "    plat_pair_global_count = sdf.rdd.map(lambda x: (x.rqst_idnumber, x.partnercode)).groupByKey().flatMap(count_global).reduceByKey(lambda x1, x2: x1+x2)\n",
    "    plat_pair_window_count = sdf.rdd.map(lambda x: (x.rqst_idnumber, x.items)).groupByKey().flatMap(count_window).reduceByKey(lambda x1, x2: x1+x2)\n",
    "    plat_pair_count = plat_pair_global_count.union(plat_pair_window_count)\n",
    "    plat_pair_count.cache()\n",
    "    def calc_ratio(row):\n",
    "        result = []\n",
    "        for line in row[1]:\n",
    "            result.append(line)\n",
    "        if len(result) == 2:\n",
    "            return (row[0], float(result[0]) / result[1] if result[1] > result[0] else float(result[1]) / result[0])\n",
    "        return (row[0], 0.0)\n",
    "    \n",
    "    plat_pair_ratio = spark.sparkContext.broadcast( dict(plat_pair_count.groupByKey().map(calc_ratio).collect()) )\n",
    "    tmp_df = spark.createDataFrame(plat_pair_ratio.value.items())\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    tmp_df.registerTempTable(tmp_table_name)\n",
    "    spark.sql( 'insert overwrite table ml.mlp_cxw_more_%s_plat_pair_ratio_dt partition (ds) select *, %s from %s' % (str(int(window)), version_number, tmp_table_name) )\n",
    "    \n",
    "    # 计算条件概率\n",
    "    def cond_prob_calc(row):\n",
    "        if len(row[0][1]) == 0: return []\n",
    "        left, right = row[0][0], row[0][1]\n",
    "        return [(left+'|'+right, float(row[1]) / bc_dict.value[right]), (right+'|'+left, float(row[1]) / bc_dict.value[left])]\n",
    "    bc_cond_prob = spark.sparkContext.broadcast( dict( df_freq.flatMap(cond_prob_calc).collect() ) )\n",
    "    \n",
    "    tmp_df = spark.createDataFrame(bc_cond_prob.value.items())\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    tmp_df.registerTempTable(tmp_table_name)\n",
    "    spark.sql( 'insert overwrite table ml.mlp_cxw_more_%s_cond_prob_dt partition (ds) select *, %s from %s' % (str(int(window)), version_number, tmp_table_name) )\n",
    "    \n",
    "    # 合并特征\n",
    "    tmp_sdf = spark.sql('select * from ml.mlp_cxw_more_%s_pgrk_result_dt where ds=%s' % (str(int(window)), version_number))  # pgrk_result_table\n",
    "    bc_pr_dict = spark.sparkContext.broadcast( dict( tmp_sdf.rdd.map( lambda x: (x[0], [ x[1], x[2], x[3] ]) ).collect() ) )\n",
    "    def merge_info(part): # sequence_id: 0, eventoccurtime: 1, rqst_idnumber: 2, partnercode: 3, deviceid: 4, rqst_customerchannel: 5, delta_time: 6, items: 7\n",
    "        result = []\n",
    "        for line in part:\n",
    "            row = list(line)\n",
    "            if len(row[7]) < 1: continue\n",
    "            if row[7] + '|' + row[3] not in bc_cond_prob.value: continue # if not enough count for pair\n",
    "            best_prob_val =  bc_cond_prob.value[row[7] + '|' + row[3]] \n",
    "            best_ratio_val = plat_pair_ratio.value[row[3] + ',' + row[7]] if row[7] > row[3] else plat_pair_ratio.value[row[7] + ',' + row[3]]\n",
    "            best_lift_val = float(bc_cond_prob.value[row[7] + '|' + row[3]]) / bc_dict.value[row[7]] * bc_dict.value['totol_count_of_platforms']\n",
    "            best_kulc_val = 0.5 * (bc_cond_prob.value[row[7] + '|' + row[3]] + bc_cond_prob.value[ row[3] + '|' + row[7]]) \n",
    "            best_ir_val = float(bc_cond_prob.value[row[7] + '|' + row[3]]) / bc_cond_prob.value[ row[3] + '|' + row[7]]\n",
    "            support_cnt = bc_dict.value[row[3] + ',' + row[7]] if row[3] < row[7] else bc_dict.value[row[7] + ',' + row[3]]\n",
    "            label = 9\n",
    "            if row[4] != None and str(row[4]) != '':\n",
    "                if row[5] != '1':\n",
    "                    label = 0\n",
    "            elif row[5] == '1':\n",
    "                label = 1\n",
    "            \n",
    "            if row[7] in bc_pr_dict.value and row[3] in bc_pr_dict.value:\n",
    "                result.append( [row[0], row[2], row[6], row[7], float(best_prob_val), support_cnt, \n",
    "                            bc_pr_dict.value[row[7]][0], bc_pr_dict.value[row[7]][1],\n",
    "                            float(best_lift_val), float(best_kulc_val), float(best_ir_val), \n",
    "                            bc_pr_dict.value[row[3]][0], bc_pr_dict.value[row[3]][1], float(best_ratio_val), \n",
    "                            bc_pr_dict.value[row[7]][2], label ]  )    \n",
    "        return result\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"_from\", StringType(), True),   \n",
    "        StructField(\"_prob\", FloatType(), True),\n",
    "        StructField(\"support_count\", IntegerType(), True),\n",
    "        StructField(\"from_rank\", FloatType(), True),        \n",
    "        StructField(\"from_cnt\", StringType(), True),\n",
    "        StructField(\"_lift_val\", FloatType(), True),\n",
    "        StructField(\"_kulc_val\", FloatType(), True),\n",
    "        StructField(\"_ir_val\", FloatType(), True),\n",
    "        StructField(\"to_rank\", FloatType(), True),        \n",
    "        StructField(\"to_cnt\", StringType(), True), \n",
    "        StructField(\"ratio_val\", FloatType(), True),\n",
    "        StructField(\"from_out_degree\", IntegerType(), True),\n",
    "        StructField(\"label\", IntegerType(), True)])\n",
    "    sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions(merge_info).toDF(schema)\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    sdf.registerTempTable(tmp_table_name)\n",
    "    spark.sql( 'insert overwrite table ml.mlp_cxw_more_%s_tbres_dt partition (ds) select *, %s from %s' % (str(int(window)), version_number, tmp_table_name) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对多个窗口的结果进行拼接，以最左的窗口为主\n",
    "'''\n",
    "def merge_train_data(spark, version_number, tablenames):\n",
    "    from pyspark.sql.functions import col\n",
    "    tablenames = ['ml.mlp_cxw_more_%s_tbres_dt' % str(int( float(name[:-3]) * 60 )) for name in tablenames ]\n",
    "    sdf1 = spark.sql('select * from %s where ds=%s' % (tablenames[0], version_number)).drop('ds')\n",
    "    for i in range(1, len(tablenames)):\n",
    "        sdf2 = spark.sql('select * from %s where ds=%s' % (tablenames[i], version_number))\n",
    "        sdf2 = sdf2.drop('deviceid').drop('rqst_idnumber').drop('_from').drop('ds')\n",
    "        for name in sdf2.columns:\n",
    "            sdf2 = sdf2.withColumnRenamed(name, name+str(i))\n",
    "        sdf1 = sdf1.join(sdf2, col('sequence_id') == col('sequence_id'+str(i)), 'left').drop('sequence_id' + str(i)).drop('label'+ str(i))\n",
    "    return sdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One class SVM模型的训练，包括scaler\n",
    "'''\n",
    "def OneClassSvmModel(spark, sdf, version_number, nu_para=0.0001, gamma_para=0.01):\n",
    "    sdf = sdf.drop('sequence_id').drop('rqst_idnumber').drop('_from')\n",
    "    df = sdf.withColumn('PR_propotion', F.col('from_rank') / F.col('to_rank') ).toPandas()\n",
    "    df = df.fillna(0)\n",
    "    # get not drainage data, split train and test\n",
    "    train_data = df[df['label']==0].drop(['label'], axis = 1)\n",
    "    train_label = pd.Series(np.ones(df[df['label']==0].label.count()))\n",
    "    train_data, test_data, y_train, y_test = train_test_split(train_data, train_label, test_size=0.3, random_state=2018)\n",
    "    # drainage and unlabeled data\n",
    "    valid_data = df[df['label']==1].drop(['label'], axis = 1)\n",
    "    unlabel_data = df[df['label']==9].drop(['label'],axis = 1)\n",
    "\n",
    "    # train and save scaler model\n",
    "    scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "    path1 = 'drainage_models/scaler_model_%s' % (version_number)\n",
    "    with open(path1, 'wb') as f:\n",
    "        joblib.dump(scaler, f)\n",
    "        savePersonalFile(f, overwrite=True)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    valid_data = scaler.transform(valid_data)\n",
    "    unlabel_data = scaler.transform(unlabel_data)\n",
    "\n",
    "    # train and save one class SVM model\n",
    "    clf = svm.OneClassSVM(nu=nu_para, kernel=\"rbf\", gamma=gamma_para)\n",
    "    clf.fit(train_data)\n",
    "    path2 = 'drainage_models/train_model_%s' % (version_number)\n",
    "    with open(path2, 'wb') as f:\n",
    "        joblib.dump(clf, f)\n",
    "        savePersonalFile(f, overwrite=True)\n",
    "    \n",
    "    # calculate prediction and distance\n",
    "    pred_train = clf.predict(train_data)\n",
    "    pred_test = clf.predict(test_data)\n",
    "    pred_valid = clf.predict(valid_data)\n",
    "    pred_unlabel = clf.predict(unlabel_data)\n",
    "\n",
    "    # ------------- Evaluation --------------------\n",
    "    recall_train = pred_train[pred_train == 1].size / pred_train.shape[0]\n",
    "    recall_test = pred_test[pred_test == 1].size / pred_test.shape[0]\n",
    "    recall_valid = 1 - pred_valid[pred_valid == 1].size / pred_valid.shape[0]\n",
    "    print(pred_train[pred_train == -1].size + pred_test[pred_test == -1].size + pred_valid[pred_valid == -1].size + pred_unlabel[pred_unlabel == -1].size)\n",
    "    print('Recall\\ntrain:%s, test:%s, vaild:%s' % (recall_train, recall_test, recall_valid))\n",
    "    print('F-meansure\\ntrain:%s, test:%s, vaild:%s' % (2*recall_train/(recall_train+1.0), 2*recall_test/(recall_test+1.0), 2*recall_valid/(recall_valid+1.0)) )\n",
    "    return [ '/user/datacompute/users/xingwei.chen/' + path1, \n",
    "             '/user/datacompute/users/xingwei.chen/' + path2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def main(spark):\n",
    "    version_number = int(time.strftime('%Y%m%d%H%M%S'), time.localtime(time.time())) # e.g. '20180901000000'\n",
    "    tmp = version_number // 1000000\n",
    "    nanotime_of_day = datetime.datetime(tmp//10000, tmp//100%100, tmp%100)\n",
    "    sql_time = int(time.mktime(nanotime_of_day.timetuple())) * 1000\n",
    "    sdf = spark.sql('''\n",
    "        select sequence_id, round(cast(eventoccurtime as bigint)/1000, 0) eventoccurtime, rqst_idnumber, partercode,\n",
    "        deviceid, rqst_customerchannel from bigdata.raw_activity_flat \n",
    "        where  \n",
    "            ( ( year=from_unixtime(cast(%s-86400l*30 as bigint)/1000, 'yyyy') \n",
    "                and month=from_unixtime(cast(%s as bigint)/1000, 'MM')\n",
    "                and day<=from_unixtime(cast(%s as bigint)/1000, 'dd') ) \n",
    "                or\n",
    "              ( year=from_unixtime(cast(%s-86400l*30 as bigint)/1000, 'yyyy') \n",
    "                and month=from_unixtime(cast(%s-86400l*30 as bigint)/1000, 'MM')\n",
    "                and day>=from_unixtime(cast(%s-86400l*30 as bigint)/1000, 'dd') ) )\n",
    "            and\n",
    "            (event = 'loan' and eventtype='entPreLoan') and rqst_idnumber is not null\n",
    "    ''' % (sql_time, sql_time, sql_time, sql_time, sql_time, sql_time))\n",
    "    windows = ['7min', '5min', '3min', '2min']\n",
    "    sdf_pr = forward_for_pr(spark, sdf, window=int(float(windows[0][:-3])*60))\n",
    "    sdf_fp = backward_for_fp(spark, sdf, window=int(float(windows[0][:-3])*60))\n",
    "    for wind in windows:\n",
    "        pagerank(spark, sdf_pr, version_number,  error_thresh = 1e-2, trans_thresh = 0.85, thresh1=0, thresh2=0.001, thresh3=100, window = float(wind[:-3]) * 60)\n",
    "        fp_growth_table(spark, sdf_fp, version_number, min_support = 100, window = float(wind[:-3]) * 60)\n",
    "    sdf = merge_train_data(spark, version_number, windows)\n",
    "    result = OneClassSvmModel(spark, sdf, version_number)\n",
    "    print('The version number for next 7 days is %s' % version_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict(T+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/bin/python2\n",
    "# set spark.sql.execution.arrow.enabled=true \n",
    "# set spark.driver.maxResultSize=5g\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, ArrayType, LongType, FloatType\n",
    "from sklearn.externals import joblib\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以最大的窗口向前获取数据，供给fp-growth等计算关联，因为我们希望将A->B中的B去除，优势在于B可能有多条\n",
    "输入:\n",
    "    sdf: 从千维度表中筛选有用columns后，得到的小表\n",
    "    window: 窗口大小，单位为秒\n",
    "输出:\n",
    "    表: [流水号，发生时间，身份证号，平台名，设备码，用户标记的引流标识，时间差，窗口内的上一条记录的平台名]\n",
    "\"\"\"\n",
    "def backward_for_fp(spark, sdf, window=420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func1(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            result.append([ row[2], list(row) ])\n",
    "        return result\n",
    "    \n",
    "    def func2(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            items = sorted(list(row[1]), key=lambda x: (x[1], x[0]))\n",
    "            # catch A B B B as 3 record, not 1 as before\n",
    "            for i in range(1, len(items)):\n",
    "                gap_i = 1\n",
    "                while(items[i][1] - items[i-gap_i][1] <= bc_window.value and i >= gap_i and items[i][3] == items[i-gap_i][3]):\n",
    "                    gap_i += 1\n",
    "                if items[i][3] != items[i-gap_i][3] and items[i][1] - items[i-gap_i][1] <= bc_window.value and i >= gap_i:\n",
    "                    result.append( items[i] + [ int(items[i][1] - items[i-gap_i][1]), items[i-gap_i][3] ])\n",
    "        return result\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"eventoccurtime\", FloatType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"deviceid\", StringType(), True),\n",
    "        StructField(\"rqst_customerchannel\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"items\", StringType(), True)])\n",
    "    sdf = sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions( func1 ).groupByKey().mapPartitions( func2 ).toDF(schema)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(spark, sdf, version_number, window=420):\n",
    "    bc_window = spark.sparkContext.broadcast(window)\n",
    "    def func(part):\n",
    "        result = []\n",
    "        for row in part:\n",
    "            if row.delta_time >= 0 and row.delta_time <= bc_window.value:\n",
    "                result.append(list(row))\n",
    "        return result\n",
    "   \n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"eventoccurtime\", FloatType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"deviceid\", StringType(), True),\n",
    "        StructField(\"rqst_customerchannel\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"items\", StringType(), True)])\n",
    "    sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions(func).toDF(schema)\n",
    "    \n",
    "    tmp_sdf = spark.sql('select * from ml.mlp_cxw_more_%s_bc_dict_dt where ds=%s' % (str(int(window)), version_number)).drop('ds')\n",
    "    bc_dict = spark.sparkContext.broadcast( dict(tmp_sdf.collect()) )\n",
    "\n",
    "    tmp_sdf = spark.sql('select * from ml.mlp_cxw_more_%s_plat_pair_ratio_dt where ds=%s' % (str(int(window)), version_number)).drop('ds')\n",
    "    plat_pair_ratio = spark.sparkContext.broadcast( dict(tmp_sdf.collect()) )\n",
    "\n",
    "    tmp_sdf = spark.sql('select * from ml.mlp_cxw_more_%s_cond_prob_dt where ds=%s' % (str(int(window)), version_number)).drop('ds')\n",
    "    bc_cond_prob =  spark.sparkContext.broadcast( dict(tmp_sdf.collect()) )\n",
    "    \n",
    "    tmp_sdf = spark.sql('select * from ml.mlp_cxw_more_%s_pgrk_result_dt where ds=%s' % (str(int(window)), version_number))  # pgrk_result_table\n",
    "    bc_pr_dict = spark.sparkContext.broadcast( dict( tmp_sdf.rdd.map( lambda x: (x[0], [ x[1], x[2], x[3] ]) ).collect() ) )\n",
    "    \n",
    "    \n",
    "    def map_features(part): # sequence_id: 0, eventoccurtime: 1, rqst_idnumber: 2, partnercode: 3, deviceid: 4, rqst_customerchannel: 5, delta_time: 6, items: 7\n",
    "        result = []\n",
    "        for line in part:\n",
    "            row = list(line)\n",
    "            if row[7] + '|' + row[3] not in bc_cond_prob.value: continue # if not enough count for pair\n",
    "            best_prob_val =  bc_cond_prob.value[row[7] + '|' + row[3]] \n",
    "            best_ratio_val = plat_pair_ratio.value[row[3] + ',' + row[7]] if row[7] > row[3] else plat_pair_ratio.value[row[7] + ',' + row[3]]\n",
    "            best_lift_val = float(bc_cond_prob.value[row[7] + '|' + row[3]]) / bc_dict.value[row[7]] * bc_dict.value['totol_count_of_platforms']\n",
    "            best_kulc_val = 0.5 * (bc_cond_prob.value[row[7] + '|' + row[3]] + bc_cond_prob.value[ row[3] + '|' + row[7]]) \n",
    "            best_ir_val = float(bc_cond_prob.value[row[7] + '|' + row[3]]) / bc_cond_prob.value[ row[3] + '|' + row[7]]\n",
    "            support_cnt = bc_dict.value[row[3] + ',' + row[7]] if row[3] < row[7] else bc_dict.value[row[7] + ',' + row[3]]\n",
    "            if row[7] in bc_pr_dict.value and row[3] in bc_pr_dict.value:\n",
    "                result.append( [row[0], row[3], row[4], row[2], row[6], row[7], float(best_prob_val), support_cnt, \n",
    "                            bc_pr_dict.value[row[7]][0], bc_pr_dict.value[row[7]][1],\n",
    "                            float(best_lift_val), float(best_kulc_val), float(best_ir_val), \n",
    "                            bc_pr_dict.value[row[3]][0], bc_pr_dict.value[row[3]][1], float(best_ratio_val), \n",
    "                            bc_pr_dict.value[row[7]][2] ] )\n",
    "        return result\n",
    "    schema = StructType([\n",
    "        StructField(\"sequence_id\", StringType(), True),\n",
    "        StructField(\"partnercode\", StringType(), True),\n",
    "        StructField(\"deviceid\", StringType(), True),\n",
    "        StructField(\"rqst_idnumber\", StringType(), True),\n",
    "        StructField(\"delta_time\", IntegerType(), True),\n",
    "        StructField(\"_from\", StringType(), True),   \n",
    "        StructField(\"_prob\", FloatType(), True),\n",
    "        StructField(\"support_count\", IntegerType(), True),\n",
    "        StructField(\"from_rank\", FloatType(), True),        \n",
    "        StructField(\"from_cnt\", StringType(), True),\n",
    "        StructField(\"_lift_val\", FloatType(), True),\n",
    "        StructField(\"_kulc_val\", FloatType(), True),\n",
    "        StructField(\"_ir_val\", FloatType(), True),\n",
    "        StructField(\"to_rank\", FloatType(), True),        \n",
    "        StructField(\"to_cnt\", StringType(), True), \n",
    "        StructField(\"ratio_val\", FloatType(), True),\n",
    "        StructField(\"from_out_degree\", IntegerType(), True)])\n",
    "    \n",
    "    \n",
    "    sdf.repartition(100)\n",
    "    sdf = sdf.rdd.mapPartitions(map_features).toDF(schema)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predict_features(spark, dataframes):\n",
    "    from pyspark.sql.functions import col\n",
    "    sdf1 = dataframes[0]\n",
    "    for i in range(1, len(dataframes)):\n",
    "        sdf2 = dataframes[i]\n",
    "        sdf2 = sdf2.drop('deviceid').drop('rqst_idnumber').drop('_from').drop('partnercode')\n",
    "        for name in sdf2.columns:\n",
    "            sdf2 = sdf2.withColumnRenamed(name, name+str(i))\n",
    "        sdf1 = sdf1.join(sdf2, col('sequence_id') == col('sequence_id'+str(i)), 'left').drop('sequence_id' + str(i))\n",
    "    return sdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(spark, sdf, version_number, partitionNum = 100):\n",
    "    bc_model = spark.sparkContext.broadcast(joblib.load('/user/datacompute/users/xingwei.chen/drainage_models/train_model_%s' % version_number))\n",
    "    bc_scaler_model = spark.sparkContext.broadcast(joblib.load('/user/datacompute/users/xingwei.chen/drainage_models/scaler_model_%s' % version_number))\n",
    "    bc_cols = spark.sparkContext.broadcast([  u'sequence_id', u'_from', u'partnercode', u'deviceid', u'rqst_idnumber', u'delta_time', u'_prob', u'support_count', u'from_rank', u'from_cnt',\n",
    "                                              u'_lift_val', u'_kulc_val', u'_ir_val', u'to_rank', u'to_cnt',\n",
    "                                              u'ratio_val', u'from_out_degree', u'delta_time1', u'_prob1',\n",
    "                                              u'support_count1', u'from_rank1', u'from_cnt1',\n",
    "                                              u'_lift_val1', u'_kulc_val1', u'_ir_val1', u'to_rank1',\n",
    "                                              u'to_cnt1', u'ratio_val1', u'from_out_degree1',\n",
    "                                              u'delta_time2', u'_prob2', u'support_count2', u'from_rank2',\n",
    "                                              u'from_cnt2', u'_lift_val2', u'_kulc_val2', u'_ir_val2',\n",
    "                                              u'to_rank2', u'to_cnt2', u'ratio_val2', u'from_out_degree2',\n",
    "                                              u'delta_time3', u'_prob3', u'support_count3', u'from_rank3',\n",
    "                                              u'from_cnt3', u'_lift_val3', u'_kulc_val3', u'_ir_val3',\n",
    "                                              u'to_rank3', u'to_cnt3', u'ratio_val3', u'from_out_degree3', 'PR_propotion'  ])\n",
    "    sdf = sdf.select(bc_cols.value[:-1])\n",
    "    sdf = sdf.withColumn('PR_propotion', F.col('from_rank') / F.col('to_rank') ) \n",
    "    sdf = sdf.repartition(partitionNum)\n",
    "    def func(part_rdd):\n",
    "        data = [row for row in part_rdd]\n",
    "        if len(data) == 0:\n",
    "            return []\n",
    "        df = pd.DataFrame(data, columns=bc_cols.value)\n",
    "        df = df.fillna(0)\n",
    "        scaled_data = bc_scaler_model.value.transform(df[bc_cols.value[5:]])\n",
    "        prediction = bc_model.value.predict(scaled_data)\n",
    "        distance = bc_model.value.decision_function( scaled_data )\n",
    "        df['distance'] = distance\n",
    "        devid_label = df['deviceid'].values\n",
    "        label = []\n",
    "        for i in range(len(prediction)):\n",
    "            label.append( -1 if devid_label[i] != 0 or prediction[i] > 0 else 1 )\n",
    "        df['label'] = label\n",
    "        return df.values.tolist()\n",
    "    \n",
    "    sdf = sdf.rdd.mapPartitions(func).toDF()\n",
    "    col_names = bc_cols.value + ['distance', 'label']\n",
    "    for i in range(len(col_names)):\n",
    "        sdf = sdf.withColumnRenamed('_'+str(i+1), col_names[i])\n",
    "    output_table = 'ml.mlp_drainage_recoginze_result_detail_dt'\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    sdf.registerTempTable(tmp_table_name)\n",
    "    spark.sql('insert overwrite table ' + output_table + \n",
    "                     ' partition (ds) select *, %s from %s' % (version_number, tmp_table_name) )\n",
    "\n",
    "    sdf = sdf.where(sdf.label > 0).select(['sequence_id', 'distance', 'label', '_from', 'partnercode', 'rqst_idnumber'])\n",
    "    tmp_table_name = 'tdl_tmp_algo_%s' % int(time.time() * 1000)\n",
    "    sdf.registerTempTable(tmp_table_name)\n",
    "    spark.sql('insert overwrite table ml.mlp_drainage_recoginze_result_final_dt' + \n",
    "                     ' partition (ds) select *, from_unixtime( substring(sequence_id, 0, 10), \\'yyyyMMdd\\') from %s' % tmp_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：因为数据安全的需求，很快要隔离业务网络和大数据平台网络，请不要在作业中访问大数据平台外部外部网络，包含http接口和mysqlmysql等\n",
    "def main(spark):\n",
    "    version_number, month, day = '20181101000000', 8, 1\n",
    "    tablenames = ['7min', '5min', '3min', '2min']\n",
    "    sdf = spark.sql('''select sequence_id, round(cast(eventoccurtime as bigint)/1000, 0) eventoccurtime, rqst_idnumber, partnercode, cast(deviceid as string) deviceid, rqst_customerchannel\n",
    "                        from bigdata.raw_activity_flat where year=2018 and month=%s and day=%s and (event ='loan'  OR eventtype = 'entPreLoan') and rqst_idnumber is not null''' % (month, day))\n",
    "    sdf = backward_for_fp(spark, sdf, window=420)\n",
    "    dataframes = []\n",
    "    for name in tablenames:\n",
    "        dataframes.append( get_features(spark, sdf, version_number, window = float(name[:-3]) * 60) )\n",
    "    sdf = merge_predict_features(spark, dataframes)\n",
    "    make_prediction(spark, sdf, version_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
