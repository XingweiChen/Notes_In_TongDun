{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [StructField(\"name\", StringType(), True),\n",
    "     StructField(\"age\", IntegerType(), True),\n",
    "     StructField(\"salary\", DoubleType(), True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=[(\"a\", 1, 100.0), (\"b\", 2, 200.0)], schema=schema)\n",
    "# 100 传给DoubleType会报错， 传100.0才行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"name\"]\n",
    "# df[\"xxx\"] 和 df.select(\"xxx\") 不同select返回的是DataFrame，直接indexing返回的是column对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='a', age=1, salary=100.0), Row(name='b', age=2, salary=200.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)  # df.head() 默认返回一个， 可加参数 n，（n如果大于data个数不会报错，返回所有对象）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, name: string, age: string, salary: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+-----------------+\n",
      "|summary|name|               age|           salary|\n",
      "+-------+----+------------------+-----------------+\n",
      "|  count|   2|                 2|                2|\n",
      "|   mean|null|               1.5|            150.0|\n",
      "| stddev|null|0.7071067811865476|70.71067811865476|\n",
      "|    min|   a|                 1|            100.0|\n",
      "|    max|   b|                 2|            200.0|\n",
      "+-------+----+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show() # f返回DataFrame的基本统计数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "spark = SparkSession.builder.appName(\"test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data = [('emp1', 'John', None), ('emp2', None, None), \n",
    "              ('emp1', None, 345.0), ('emp1', 'Cindy', 456.0)], schema = StructType([StructField(\"pos\", StringType(), True),\n",
    "     StructField(\"name\", StringType(), True),\n",
    "     StructField(\"salary\", DoubleType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|  null|\n",
      "|emp2| null|  null|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df\n",
    "df1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|  null|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df\n",
    "df2.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df\n",
    "df3.na.drop(subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+----------+------+\n",
      "| pos|      name|salary|\n",
      "+----+----------+------+\n",
      "|emp1|      John|  null|\n",
      "|emp2|FILL VALUE|  null|\n",
      "|emp1|FILL VALUE| 345.0|\n",
      "|emp1|     Cindy| 456.0|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill('FILL VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John|   0.0|\n",
      "|emp2| null|   0.0|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "| pos|   name|salary|\n",
      "+----+-------+------+\n",
      "|emp1|   John|  null|\n",
      "|emp2|No name|  null|\n",
      "|emp1|No name| 345.0|\n",
      "|emp1|  Cindy| 456.0|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.na.fill('No name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  avg|\n",
      "+-----+\n",
      "|400.5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_val = df.select(mean(df['salary']).alias('avg')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "| pos| name|salary|\n",
      "+----+-----+------+\n",
      "|emp1| John| 400.5|\n",
      "|emp2| null| 400.5|\n",
      "|emp1| null| 345.0|\n",
      "|emp1|Cindy| 456.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(df.select(mean(df['salary'])).collect()[0][0], subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark with dates and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear, \n",
    "                                   month, year, weekofyear, format_number, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('dates_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
